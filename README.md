<div align="center">

<img src="assets/fig3.png" alt="ConHGNN-SUM Logo" width="400"/>

# ConHGNN-SUM: Contextualized Heterogeneous Graph Neural Networks for Extractive Document Summarization

</div>

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.5+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.0+-ee4c2c.svg)](https://pytorch.org/)
[![DGL](https://img.shields.io/badge/DGL-0.4-orange.svg)](https://dgl.ai/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Paper](https://img.shields.io/badge/Paper-IEEE%20AISP%202024-red.svg)](https://ieeexplore.ieee.org/document/10475307)

*Revolutionizing document summarization through heterogeneous graph neural networks*

[ğŸ“– Paper](https://ieeexplore.ieee.org/document/10475307) â€¢ [ğŸš€ Quick Start](#quick-start) â€¢ [ğŸ’« Results](#results) â€¢ [ğŸ”§ Installation](#installation)

</div>

---

## ğŸ“– Paper

**ConHGNN-SUM** is published in the **2024 20th CSI International Symposium on Artificial Intelligence and Signal Processing (AISP)**

ğŸ“„ **Read the full paper**: [ConHGNN-SUM: A Contextualized Heterogeneous Graph Neural Network for Extractive Text Summarization](https://ieeexplore.ieee.org/document/10475307)

---

## ğŸŒŸ Overview

**ConHGNN-SUM** represents a breakthrough in extractive document summarization, leveraging the power of **Heterogeneous Graph Neural Networks** to capture complex relationships between words and sentences. Unlike traditional methods that treat documents as sequential text, our approach models documents as dynamic graphs where semantic relationships drive the summarization process.

### âœ¨ Key Innovations

- ğŸ”— **Heterogeneous Graph Architecture**: Novel word-sentence graph representation that captures semantic relationships
- ğŸ¯ **Multi-Scale Feature Fusion**: CNN-based n-gram extraction combined with BiLSTM encoding
- ğŸ§® **Graph Attention Mechanism**: Advanced GAT layers for refined semantic node representations
- ğŸ”„ **Iterative Message Passing**: Cross-node feature injection for enhanced context understanding
- ğŸ“Š **State-of-the-Art Performance**: Superior results on CNN/DailyMail benchmark

---

## ğŸ§¬ Methodology

Our approach transforms document summarization into a graph-based learning problem through a sophisticated four-stage pipeline:

### ğŸ“Š Model Architecture Overview

<div align="center">
<img src="assets/fig1.png" alt="Model Overview" width="400"/>

*Figure 1: Complete system architecture showing the four primary components: Graph Initializers, Heterogeneous Graph Layer, Contextualized Section, and Sentence Selector*
</div>

### ğŸ”„ Dynamic Graph Update Process

<div align="center">
<img src="assets/fig2.png" alt="Graph Update Process" width="300"/>

*Figure 2: Iterative message passing mechanism for updating word and sentence node representations within the heterogeneous graph*
</div>

### ğŸ” Technical Deep Dive

#### 1. **Document Preprocessing & Representation**
- ğŸ“ **Input Handling**: Process up to 50 sentences per document (100 words per sentence)
- ğŸ¨ **Word Embeddings**: 300-dimensional GloVe-initialized vectors
- ğŸ§© **N-gram Extraction**: Multi-kernel CNN (2,3,4,5,6,7-grams) for rich feature capture
- ğŸ”„ **Sequential Encoding**: Bidirectional LSTM for contextual word representations

#### 2. **Heterogeneous Graph Construction**
- ğŸ”´ **Word Nodes**: Individual words with 300-dimensional features
- ğŸŸ¢ **Sentence Nodes**: Aggregated representations with 64-dimensional features  
- âš¡ **Edge Weights**: TF-IDF based connectivity between words and sentences
- ğŸ¯ **Graph Topology**: Document-specific graphs with semantic relationships

#### 3. **Graph Neural Network Processing**
- ğŸ§  **Graph Attention Networks**: Semantic refinement of node representations
- ğŸ’¬ **Message Passing**: Bidirectional feature exchange between word and sentence nodes
- ğŸ”„ **Iterative Updates**: Controllable multi-stage influence propagation
- ğŸª **Cross-Modal Fusion**: Word characteristics influence sentence selection

#### 4. **Extractive Selection**
- ğŸ¯ **Contextualized Features**: Enhanced sentence representations
- ğŸ“Š **Classification**: Binary selection for summary-worthy sentences
- âš–ï¸ **Optimization**: End-to-end training with extractive labels

---

## ğŸš€ Quick Start

### ğŸ”§ Installation

```bash
# Clone the repository
git clone https://github.com/erfan-nourbakhsh/ConHGNN-SUM.git
cd ConHGNN-SUM

# Install dependencies
pip install torch>=1.0.0
pip install dgl==0.4
pip install rouge==1.0.0
pip install pyrouge==0.1.3
pip install nltk numpy sklearn
```

### ğŸ“Š Data Preparation

We provide preprocessed **CNN/DailyMail** datasets:

- ğŸ”— **TF-IDF Features**: [Download here](https://drive.google.com/open?id=1oIYBwmrB9_alzvNDBtsMENKHthE9SW9z)
- ğŸ“„ **JSON Datasets**: [Download here](https://drive.google.com/open?id=1JW033KefyyoYUKUFj6GqeBFZSHjksTfr)

**Data Format Example:**
```json
{
  "text": ["deborah fuller has been banned from keeping animals...", "a dog breeder and exhibitor..."],
  "summary": ["warning: ... at a speed of around 30mph", "she was banned from..."],
  "label": [1, 3, 6]
}
```

**Prepare your dataset:**
```bash
# Process raw data into graph format
bash PrepareDataset.sh
```

---

## ğŸƒâ€â™‚ï¸ Training

Launch training with our optimized configuration:

```bash
python train.py \
    --cuda --gpu 0 \
    --data_dir <path/to/json/dataset> \
    --cache_dir <cache/directory> \
    --embedding_path <glove/path> \
    --model HSG \
    --save_root <model/save/path> \
    --log_root <log/path> \
    --lr_descent --grad_clip -m 3
```

### ğŸ¯ Model Variants
- **HSG**: Heterogeneous Graph with Standard attention
- **HDSG**: Heterogeneous Graph with Dense attention

---

## ğŸ“ˆ Evaluation

Evaluate your trained models:

```bash
python evaluation.py \
    --cuda --gpu 0 \
    --data_dir <path/to/json/dataset> \
    --cache_dir <cache/directory> \
    --embedding_path <glove/path> \
    --model HSG \
    --save_root <model/path> \
    --log_root <log/path> \
    -m 3 --test_model multi \
    --use_pyrouge
```

### ğŸ”§ Evaluation Options

| Option | Description | Default |
|--------|-------------|---------|
| `--use_pyrouge` | Use pyrouge for evaluation | `False` |
| `--limit` | Limit output to gold summary length | `False` |
| `--blocking` | Enable trigram blocking | `False` |
| `--save_label` | Save labels without ROUGE calculation | `False` |

---

## ğŸ’« Results

### ğŸ† Pre-trained Models

We provide state-of-the-art checkpoints:

- ğŸ“¦ **CNN/DailyMail Checkpoints**: [Download here](https://drive.google.com/file/d/14Z8fhglg7BVkl2qiQDFFkBtrJ5BkjVEz/view)
- ğŸ“Š **Model Outputs**: [Download here](https://drive.google.com/file/d/176m98y4Dxgn7UcZ7OUxxoXsLwVxxA75a/view)

### ğŸ“Š Performance Metrics

Our model achieves competitive results on standard benchmarks with significant improvements in semantic coherence and factual accuracy.

---

## ğŸ› ï¸ ROUGE Installation

For accurate evaluation, install ROUGE environment:

```bash
# Install system dependencies
sudo apt-get install libxml-perl libxml-dom-perl

# Install pyrouge
pip install git+git://github.com/bheinzerling/pyrouge

# Configure ROUGE
export PYROUGE_HOME_DIR=/path/to/RELEASE-1.5.5
pyrouge_set_rouge_path $PYROUGE_HOME_DIR
chmod +x $PYROUGE_HOME_DIR/ROUGE-1.5.5.pl

# Build WordNet 2.0
cd $PYROUGE_HOME_DIR/data/WordNet-2.0-Exceptions/
./buildExeptionDB.pl . exc WordNet-2.0.exc.db
cd ../
ln -s WordNet-2.0-Exceptions/WordNet-2.0.exc.db WordNet-2.0.exc.db
```

---

## ğŸ”— Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| **Python** | 3.5+ | Core runtime |
| **PyTorch** | 1.0+ | Deep learning framework |
| **DGL** | 0.4 | Graph neural networks |
| **rouge** | 1.0.0 | Evaluation metrics |
| **pyrouge** | 0.1.3 | ROUGE evaluation |
| **nltk** | latest | Text processing |
| **numpy** | latest | Numerical computing |
| **sklearn** | latest | Machine learning utilities |

---

## ğŸ“„ Citation

If you use ConHGNN-SUM in your research, please cite:

```bibtex
@inproceedings{10475307,
  title={ConHGNN-SUM: A Contextualized Heterogeneous Graph Neural Network for Extractive Text Summarization}, 
  author={Nourbakhsh, Seyed Erfan and Kashani, Hamidreza Baradaran},
  booktitle={2024 20th CSI International Symposium on Artificial Intelligence and Signal Processing (AISP)}, 
  year={2024},
  pages={1--8},
  doi={10.1109/AISP61396.2024.10475307}
}
```

---

## ğŸ“§ Contact

- ğŸ‘¤ **Author**: [Erfan Nourbakhsh](mailto:erfan.nourbakhsh@my.utsa.edu)
- ğŸŒ **Project Link**: [https://github.com/erfan-nourbakhsh/ConHGNN-SUM](https://github.com/erfan-nourbakhsh/ConHGNN-SUM)
- ğŸ“ **Issues**: [Report bugs or request features](https://github.com/erfan-nourbakhsh/ConHGNN-SUM/issues)

---

<div align="center">

**â­ Star this repository if you find it helpful!**

*Made with â¤ï¸ for the NLP research community*

</div>